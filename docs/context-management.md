# Управление контекстом диалога

## Команда `/forget`

Команда `/forget` позволяет пользователю начать диалог "с чистого листа", не удаляя историю сообщений из базы данных.

### Как работает

1. **После выполнения `/forget`:**
   - `active_messages_count` устанавливается в `0`
   - `remind_of_yourself` устанавливается в `"0"` (отключаются напоминания)
   - Все старые сообщения остаются в БД для статистики
   - При следующем запросе в LLM передается только системный промпт (без истории)

2. **После первого сообщения пользователя:**
   - Сообщение сохраняется в БД
   - LLM получает запрос БЕЗ контекста (так как `active_messages_count = 0`)
   - Ответ LLM сохраняется в БД
   - `active_messages_count` увеличивается до `2`

3. **После второго сообщения:**
   - Сообщение сохраняется в БД
   - LLM получает запрос С контекстом (последние 2 сообщения: предыдущая пара user-assistant)
   - Ответ LLM сохраняется в БД
   - `active_messages_count` увеличивается до `4`

4. **И так далее:**
   - С каждой парой сообщений (user + assistant) счетчик увеличивается на 2
   - В контекст LLM попадают только последние `active_messages_count` сообщений
   - Максимальное количество ограничено `MAX_CONTEXT` (из .env)

### Поле `active_messages_count`

Это поле управляет количеством сообщений, передаваемых в контекст LLM:

- **`NULL`** (по умолчанию): используются последние `MAX_CONTEXT` сообщений из всей истории
- **`0`**: контекст пустой, следующий ответ будет без учета истории
- **`N > 0`**: в контекст передаются последние N сообщений (но не больше `MAX_CONTEXT`)

### Примеры

#### Пример 1: Обычное общение (без `/forget`)

```
User: active_messages_count = NULL

Сообщение 1: "Привет"
→ В контекст: последние MAX_CONTEXT сообщений из истории

Сообщение 2: "Как дела?"
→ В контекст: последние MAX_CONTEXT сообщений из истории (включая "Привет" и ответ)
```

#### Пример 2: Общение после `/forget`

```
User: выполняет /forget
→ active_messages_count = 0

Сообщение 1: "Привет"
→ В контекст: [] (пусто)
→ После ответа: active_messages_count = 2

Сообщение 2: "Как дела?"
→ В контекст: ["Привет", "Ответ_1"]
→ После ответа: active_messages_count = 4

Сообщение 3: "Отлично!"
→ В контекст: ["Привет", "Ответ_1", "Как дела?", "Ответ_2"]
→ После ответа: active_messages_count = 6
```

### Код

Логика увеличения счетчика находится в `services/llm_service.py`:

```python
# Увеличиваем счетчик активных сообщений (если он используется)
# +2 потому что добавили пару: user message + assistant message
if user.active_messages_count is not None:
    user.active_messages_count += 2
    logger.debug(f"USER{chat_id} active_messages_count увеличен до {user.active_messages_count}")
```

Логика получения контекста находится в `database.py`:

```python
async def get_context_for_llm(self):
    """
    Возвращает сообщения для отправки в LLM с учетом active_messages_count:
    - NULL: возвращает последние MAX_CONTEXT сообщений
    - 0: возвращает пустой список (забыть всё)
    - N: возвращает последние N сообщений (но не больше MAX_CONTEXT)
    """
```

## Связанные команды

- **`/mute`**: отключает напоминания, но НЕ влияет на контекст диалога
- **`/start`**: приветствие, НЕ влияет на контекст
- **`/help`**: справка, НЕ влияет на контекст

## Технические детали

### Почему счетчик увеличивается на 2?

Каждый обмен сообщениями состоит из двух частей:
1. Сообщение пользователя (`role: "user"`)
2. Ответ ассистента (`role: "assistant"`)

Обе сохраняются в БД как отдельные записи, поэтому счетчик увеличивается на 2.

### Зачем хранить старые сообщения после `/forget`?

Старые сообщения сохраняются для:
1. Статистики использования бота
2. Возможной будущей функциональности (например, восстановление контекста)
3. Анализа качества ответов LLM
4. Отладки и логирования

### MAX_STORAGE vs MAX_CONTEXT

- **`MAX_STORAGE`**: максимальное количество сообщений в БД (по умолчанию 100)
  - Когда достигается лимит, старые сообщения удаляются
  - Это ограничение на размер БД

- **`MAX_CONTEXT`**: максимальное количество сообщений для LLM (по умолчанию 10)
  - Ограничивает размер промпта, отправляемого в API
  - Это ограничение на размер контекста для модели

